{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported docker successfully.\n"
     ]
    }
   ],
   "source": [
    "import docker\n",
    "print(\"imported docker successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont=['hive-server','namenode','datanode','historyserver','hive-metastore-postgresql'\n",
    ",'presto-coordinator','hive-metastore','nodemanager','resourcemanager']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping all the running containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container:datanode stopped.\n",
      "container:historyserver stopped.\n",
      "container:resourcemanager stopped.\n",
      "container:namenode stopped.\n",
      "container:nodemanager stopped.\n"
     ]
    }
   ],
   "source": [
    "client=docker.from_env()\n",
    "for container in client.containers.list():\n",
    "    print(\"container:{} stopped.\".format(container.name))\n",
    "    container.stop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting required containers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container:hive-server started.\n",
      "container:datanode started.\n",
      "container:historyserver started.\n",
      "container:hive-metastore-postgresql started.\n",
      "container:resourcemanager started.\n",
      "container:namenode started.\n",
      "container:nodemanager started.\n",
      "container:presto-coordinator started.\n",
      "container:hive-metastore started.\n",
      "all live containers:\n",
      "hive-server 0a705452ffd0fb47a60689d581e2d6e38c4fb82a6d260d379f9aee646f88e4b4\n",
      "datanode 090a9d330f84cc4a6968e4f9a5e2e65dbff558566749fb63b35710a4f193d0fa\n",
      "historyserver 695c419b5345a90269822a6d327a14234c6ffbc1934afd68593b18d244556868\n",
      "hive-metastore-postgresql b53bcda67f391c98538477775d3ecd9a3b08f614d80498befd1046e6d3bf453f\n",
      "resourcemanager a3ab9e4776cc91f45ddc1af40618ad972b670a81f3ba07fc0c5845e1877b00c7\n",
      "namenode 3014cb7bf7745fc2b35ab7286eafed5b4246897ca32dd3ee55783631116714e3\n",
      "nodemanager 86fe60ec723efbdfbf5e32f3dd3504a7487e48a370637fe5ec06e27fc95afecd\n",
      "presto-coordinator 82580ab4c7e8faec3e4c3006f43005a915ab52aaa628f345fdd7e3310eba9e82\n",
      "hive-metastore 8e7b59c62861435d217829617d5aa1b3c94c84b3c499c8bb806938a876f0101e\n"
     ]
    }
   ],
   "source": [
    "for container in client.containers.list(True):\n",
    "    if container.name in cont:\n",
    "        print(\"container:{} started.\".format(container.name))\n",
    "        container.start()\n",
    "print(\"all live containers:\")\n",
    "for container in client.containers.list():\n",
    "    print(container.name,container.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExecResult(exit_code=0, output=b'Found 4 items\n",
      "drwxr-xr-x   - root supergroup          0 2023-05-11 19:27 /data\n",
      "drwxr-xr-x   - root supergroup          0 2023-04-25 08:07 /rmstate\n",
      "drwxrwxr-x   - root supergroup          0 2023-04-25 20:35 /tmp\n",
      "drwxr-xr-x   - root supergroup          0 2023-04-25 20:35 /user\n",
      "')\n"
     ]
    }
   ],
   "source": [
    "cobj=[]\n",
    "for container in client.containers.list():\n",
    "    if container.name == 'hive-server':\n",
    "        cobj.append(container)\n",
    "i=str(cobj[0].exec_run('hdfs dfs -ls /')).split('\\\\n')\n",
    "for p in i:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docker.models.containers.Container"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cobj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExecResult(exit_code=0, output=b'Found 4 items\n"
     ]
    }
   ],
   "source": [
    "for p in i[0].split('\\\\n'):\n",
    "    print(p)\n",
    "    #print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExecResult(exit_code=0, output=b'SLF4J: Class path contains multiple SLF4J bindings.\\nSLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: Found binding in [jar:file:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n\\nLogging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cobj[0].exec_run(\"hive -e 'set hive.cli.print.header=true;'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['OK']\n",
      "['Time taken: 1.989 seconds']\n",
      "['OK']\n",
      "['Time taken: 0.166 seconds']\n",
      "[\"')\"]\n"
     ]
    }
   ],
   "source": [
    "#DROP TABLES\n",
    "i=str(cobj[0].exec_run\\\n",
    "(\"\"\"hive -e 'use hive_db; drop table sales_order_csv_orc_v1_backup'\n",
    "\"\"\")).split('true')\n",
    "for line in i[1].split('\\\\n'):\n",
    "    print(line.split('\\\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExecResult(exit_code=0, output=b'SLF4J: Class path contains multiple SLF4J bindings.\\nSLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: Found binding in [jar:file:/opt/hadoop-2.7.4/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\nSLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n\\nLogging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\nOK\\nTime taken: 6.843 seconds\\nWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\\nQuery ID = root_20230515114822_c977d955-ec76-4346-a7c9-cea5fa6488db\\nTotal jobs = 1\\nLaunching Job 1 out of 1\\nNumber of reduce tasks determined at compile time: 1\\nIn order to change the average load for a reducer (in bytes):\\n  set hive.exec.reducers.bytes.per.reducer=<number>\\nIn order to limit the maximum number of reducers:\\n  set hive.exec.reducers.max=<number>\\nIn order to set a constant number of reducers:\\n  set mapreduce.job.reduces=<number>\\nJob running in-process (local Hadoop)\\n2023-05-15 11:48:32,618 Stage-1 map = 0%,  reduce = 0%\\n2023-05-15 11:48:33,674 Stage-1 map = 100%,  reduce = 0%\\n2023-05-15 11:48:34,689 Stage-1 map = 100%,  reduce = 100%\\nEnded Job = job_local20496503_0001\\nMoving data to directory hdfs://namenode:9000/user/hive/warehouse/hive_db.db/sales_order_csv_orc_v1_backup_1\\nMapReduce Jobs Launched: \\nStage-Stage-1:  HDFS Read: 104128 HDFS Write: 2572 SUCCESS\\nTotal MapReduce CPU Time Spent: 0 msec\\nOK\\nTime taken: 16.906 seconds\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREATE SUB-TABLES\n",
    "cobj[0].exec_run\\\n",
    "(\"\"\"hive -e \"use hive_db; create table sales_order_csv_orc_v1_backup as \n",
    "select * from sales_order_data_csv_v1_orc where status='Shipped' limit 20;\" \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n",
      "['OK']\n",
      "['sales_order_csv_orc.ordernumber', 'sales_order_csv_orc.quantityordered', 'sales_order_csv_orc.priceeach', 'sales_order_csv_orc.orderlinenumber', 'sales_order_csv_orc.sales', 'sales_order_csv_orc.status', 'sales_order_csv_orc.qtr_id', 'sales_order_csv_orc.month_id', 'sales_order_csv_orc.year_id', 'sales_order_csv_orc.productline', 'sales_order_csv_orc.msrp', 'sales_order_csv_orc.productcode', 'sales_order_csv_orc.phone', 'sales_order_csv_orc.city', 'sales_order_csv_orc.state', 'sales_order_csv_orc.postalcode', 'sales_order_csv_orc.country', 'sales_order_csv_orc.territory', 'sales_order_csv_orc.contactlastname', 'sales_order_csv_orc.contactfirstname', 'sales_order_csv_orc.dealsize']\n",
      "['10159', '49', '100.0', '14', '5205.27', 'Shipped', '4', '10', '2003', 'Motorcycles', '95', 'S10_1678', '6505551386', 'San Francisco', 'CA', '', 'USA', 'NA', 'Brown', 'Julie', 'Medium']\n",
      "['10201', '22', '98.57', '2', '2168.54', 'Shipped', '4', '12', '2003', 'Motorcycles', '95', 'S10_1678', '6505555787', 'San Francisco', 'CA', '', 'USA', 'NA', 'Murphy', 'Julie', 'Small']\n",
      "['10333', '26', '100.0', '3', '3003.0', 'Shipped', '4', '11', '2004', 'Classic Cars', '214', 'S10_1949', '6505555787', 'San Francisco', 'CA', '', 'USA', 'NA', 'Murphy', 'Julie', 'Medium']\n",
      "['10381', '36', '100.0', '3', '8254.8', 'Shipped', '1', '2', '2005', 'Classic Cars', '214', 'S10_1949', '6505551386', 'San Francisco', 'CA', '', 'USA', 'NA', 'Brown', 'Julie', 'Large']\n",
      "['10159', '37', '100.0', '17', '5016.83', 'Shipped', '4', '10', '2003', 'Motorcycles', '118', 'S10_2016', '6505551386', 'San Francisco', 'CA', '', 'USA', 'NA', 'Brown', 'Julie', 'Medium']\n",
      "['10201', '24', '100.0', '5', '3025.92', 'Shipped', '4', '12', '2003', 'Motorcycles', '118', 'S10_2016', '6505555787', 'San Francisco', 'CA', '', 'USA', 'NA', 'Murphy', 'Julie', 'Medium']\n",
      "['10159', '22', '100.0', '16', '4132.7', 'Shipped', '4', '10', '2003', 'Motorcycles', '193', 'S10_4698', '6505551386', 'San Francisco', 'CA', '', 'USA', 'NA', 'Brown', 'Julie', 'Medium']\n",
      "['10201', '49', '100.0', '4', '8065.89', 'Shipped', '4', '12', '2003', 'Motorcycles', '193', 'S10_4698', '6505555787', 'San Francisco', 'CA', '', 'USA', 'NA', 'Murphy', 'Julie', 'Large']\n",
      "['10384', '34', '100.0', '4', '4846.7', 'Shipped', '1', '2', '2005', 'Classic Cars', '136', 'S10_4757', '6505551386', 'San Francisco', 'CA', '', 'USA', 'NA', 'Brown', 'Julie', 'Medium']\n",
      "['10381', '37', '100.0', '6', '6231.54', 'Shipped', '1', '2', '2005', 'Classic Cars', '147', 'S10_4962', '6505551386', 'San Francisco', 'CA', '', 'USA', 'NA', 'Brown', 'Julie', 'Medium']\n",
      "['Time taken: 11.104 seconds, Fetched: 10 row(s)']\n",
      "[\"')\"]\n"
     ]
    }
   ],
   "source": [
    "#FETCHING ROWS FROM A TABLE\n",
    "i=str(cobj[0].exec_run\\\n",
    "(\"\"\"hive -e \"set hive.cli.print.header=true;\n",
    "select * from sales_order_csv_orc where city ='San Francisco' limit 10;\" \"\"\"))\\\n",
    ".split('true')\n",
    "\n",
    "for line in i[1].split('\\\\n'):\n",
    "    print(line.split('\\\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sasl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m     18\u001b[0m \u001b[39m# Call above function\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m output \u001b[39m=\u001b[39m hiveconnection(host_name, port, user, database)\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mhiveconnection\u001b[1;34m(host_name, port, user, database)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhiveconnection\u001b[39m(host_name, port, user, database):\n\u001b[1;32m---> 10\u001b[0m     conn \u001b[39m=\u001b[39m hive\u001b[39m.\u001b[39;49mConnection(host\u001b[39m=\u001b[39;49mhost_name, port\u001b[39m=\u001b[39;49mport, username\u001b[39m=\u001b[39;49muser,\n\u001b[0;32m     11\u001b[0m                            database\u001b[39m=\u001b[39;49mdatabase)\n\u001b[0;32m     12\u001b[0m     cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m     13\u001b[0m     cur\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mshow databases;\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\hive_connect\\hiveenv\\lib\\site-packages\\pyhive\\hive.py:203\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, host, port, scheme, username, database, auth, configuration, kerberos_service_name, password, check_hostname, ssl_cert, thrift_transport)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport \u001b[39m=\u001b[39m thrift\u001b[39m.\u001b[39mtransport\u001b[39m.\u001b[39mTTransport\u001b[39m.\u001b[39mTBufferedTransport(socket)\n\u001b[0;32m    201\u001b[0m \u001b[39melif\u001b[39;00m auth \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mLDAP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mKERBEROS\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNONE\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCUSTOM\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    202\u001b[0m     \u001b[39m# Defer import so package dependency is optional\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msasl\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mthrift_sasl\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     \u001b[39mif\u001b[39;00m auth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mKERBEROS\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    207\u001b[0m         \u001b[39m# KERBEROS mode in hive.server2.authentication is GSSAPI in sasl library\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sasl'"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "host_name = \"172.19.0.7\"\n",
    "port = 10000\n",
    "user = \"root\"\n",
    "password = \"password\"\n",
    "database=\"hive_db\"\n",
    "\n",
    "def hiveconnection(host_name, port, user, database):\n",
    "    conn = hive.Connection(host=host_name, port=port, username=user,\n",
    "                           database=database)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"show databases;\")\n",
    "    result = cur.fetchall()\n",
    "\n",
    "    return result\n",
    "\n",
    "# Call above function\n",
    "output = hiveconnection(host_name, port, user, database)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiveenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
