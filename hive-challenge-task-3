Scenario Based questions:

Will the reducer work or not if you use “Limit 1” in any HiveQL query?
--- No,since limit clause is used to filter the final results of any query and doesn't involve any computation.

Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
--- if we have mulltiple clients trying to access hive at the same time then fetching data or updating data in the metastore would be comparatively slower     and the overall load would increase leading to a higher probability of hive cluster going down.since we are using default metastore configuration then     all of the metadata would be lost if the hive cluster goes down,thus leading to a loss of data.

Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
--- In order to calculate the total revenue generated for each month, we need to sum the revenue across the months present in the data.Hence this would 	    involve group by clause and one aggregate function which is sum().In order to solve the problem we can use the following steps:
    a.) since group by is involved,reducer tasks will be generated so we can try to increase the no. of reducer tasks required to execute the query.
    b.) Partitioning can be used to store data for each month in a seperate partition so instead of scanning the full table it will gather the data for each     month from the respective partition.
    c.) the data can be store in a file format that supports faster read operations like parquet so that deserialization can be done faster.

How can you add a new partition for the month December in the above partitioned table?
--- insert overwrite table transaction_details_static_partition partition(month = 'December') select cust_id,amount,country from transaction_details where       month='December';

I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
--- for this, we need to set the hive.exec.dynamic.partition.mode to 'nonstrict'
    i.e. set hive.exec.dynamic.partition.mode='nonstrict'


Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
--- CREATE TABLE SAMPLE_DATA (
	ID int,
	FIRST_NAME string,
	LAST_NAME string,
	EMAIL string,
	GENDER string,
	IP_ADDRESS string,
	)
	row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
	WITH SERDEPROPERTIES (
	"separatorChar" = ",",
	"quoteChar" = "\""
	"escapeChar" = "\\"
	STORED AS TEXTFILE
	LOCATION "/temp"
	TBLPROPERTIES("skip.header.line.count" = "1");
	

Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
--- we can store the data in a table following sequence file format as it clubs multiple small files together in a block or we can use regex expressions     while passing the file name so that only specific files which follow a specific naming pattern(timestamp) gets inserted into hive at a time.


LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
--- the above statement failed because the inputpath is incorrect as any local filepath should start with file://.hence the correct filepath would be          'file:///Home/country/state'

Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
--- Yes it is possible to add 100 more nodes thus horizontally scaling the entire cluster by adding 100 more commodity hardware machines but it would be too     expensive computationally to maintain these 200 nodes as it would be heavy overhead for the namenode to maintain the metadata related to a high number     of nodes.




Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)
--- select * from customers c left join orders o on c.id=o.customer_id;
--- select * from customers c right join orders o on c.id=o.customer_id;
--- select * from customers c inner join orders o on c.id=o.customer_id;
--- select * from customers c full outer join orders o on c.id=o.customer_id;
--- select * from customers c join orders o on c.id=o.customer_id;

BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset
	create table airquality
	(
	dates string,
	time string,
	CO_GT float,
	PT08_S1_CO integer,
	NMHC_GT integer,
	C6H6_GT float,
	PT08_S2_NMHC integer,
	NOx_GT integer,
	PT08_S3_NOx integer,
	NO2_GT integer,
	PT08_S4_NO2 integer,
	PT08_S5_O3 integer,
	T float,
	RH float,
	AH float
	) 
	row format delimited
	fields terminated by ","
	location '/data/airquality/'
	tblproperties("skip.header.line.count"="1")

2. try to place a data into table location
	done in first question

3. Perform a select operation .
select * from airquality where AH>0.1;
 
4. Fetch the result of the select operation in your local as a csv file . 
	insert overwrite directory '/data/result_01' row format delimited fields terminated by ',' select * from airquality limit 5;
	hdfs dfs -get /data/result_01/000000_0 /result

5. Perform group by operation . 
	select dates,avg(co_gt) as avg_co_gt from airquality group by dates;

7. Perform filter operation at least 5 kinds of filter examples . 
	select * from airquality where co_gt>2.0;
	select * from airquality where dates ='04-04-2005';
	select * from airquality where c6h6_gt=11.0;
	select * from airquality where nmhc_gt<0;
	select * from airquality where AH>0.1;

8. show and example of regex operation
	select dates,time,case when regexp_extract(time,'(^[2-5]*)(:)',1) = '' then 'no match' else regexp_extract(time,'(^[2-5]*)(:)',1) end as regx from 	airquality_csv_data limit 30;

9. alter table operation 
	alter table airquality rename to airquality_csv_data;

10 . drop table operation
	drop table airquality;

12 . order by operation . 
	select * from airquality order by nox_gt desc limit 20;

13 . where clause operations you have to perform . 
	select * from airquality where dates ='04-04-2005';

14 . sorting operation you have to perform .
	select * from airquality_csv_data where co_gt>5.0 sort by co_gt desc;
 
15 . distinct operation you have to perform . 
	select distinct dates from airquality;

16 . like an operation you have to perform . 
	select * from airquality where time like ('18%');

17 . union operation you have to perform . 
	select a.dates,a.time,a.co_gt,a.pt08_s1_co from airquality a where pt08_s1_co>1250;
	union all
	select b.dates,b.time,b.co_gt,b.pt08_s1_co from airquality b where co_gt>3.0;

18 . table view operation you have to perform . 
	create table airquality_csv_data_04mar2004 as select * from airquality_csv_data where dates='04-04-2004';



hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations
--- https://github.com/sameer-21B/Assignments/blob/main/hiveconnect.ipynb
